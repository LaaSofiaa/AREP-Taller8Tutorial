# -*- coding: utf-8 -*-
"""AREP- TALLER8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-G8jC34HYLJzwZ7zUt8TPH2gfYXxbI4E
"""

pip install langchain

import getpass
import os

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass()

pip install -qU "langchain[openai]"

import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4o-mini", model_provider="openai")

from langchain_core.messages import HumanMessage, SystemMessage

messages = [
    SystemMessage("Translate the following from English into spanish"),
    HumanMessage("hi!"),
]

model.invoke(messages)

model.invoke("Hello")

model.invoke([{"role": "user", "content": "Hello"}])

model.invoke([HumanMessage("Hello")])

for token in model.stream(messages):
    print(token.content, end="|")

from langchain_core.prompts import ChatPromptTemplate

system_template = "Translate the following from English into {language}"

prompt_template = ChatPromptTemplate.from_messages(
    [("system", system_template), ("user", "{text}")]
)

prompt = prompt_template.invoke({"language": "Spanish", "text": "hi!"})

prompt

prompt.to_messages()

response = model.invoke(prompt)
print(response.content)

"""Esto fue Pre-Lab Preparation la parte de arriba

ahora va https://python.langchain.com/docs/tutorials/rag/
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph

import getpass
import os

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass()

pip install -qU "langchain[openai]"

import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")

pip install -qU langchain-openai

import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

pip install -qU langchain-core

pip install -qU langchain-chroma

from langchain_openai import OpenAIEmbeddings

!pip install -qU langchain langchain-openai chromadb tiktoken

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# Generar embeddings con OpenAI
embedding_function = OpenAIEmbeddings()

# Guardar en una base de datos vectorial (ChromaDB)
vector_db = Chroma.from_documents(documents, embedding_function)

!pip install -U langchain-openai

!pip install -U langchain-openai

from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.schema import Document

# Lista de documentos de ejemplo
documents = [
    Document(page_content="LangChain is a framework for building AI applications."),
    Document(page_content="It provides tools for working with LLMs and vector databases."),
    Document(page_content="OpenAI's GPT models can be integrated into LangChain."),
]

# Generar embeddings con OpenAI
embedding_function = OpenAIEmbeddings()

# Guardar en una base de datos vectorial (ChromaDB)
vector_db = Chroma.from_documents(documents, embedding_function)

# Realiza una búsqueda de prueba
query = "What is LangChain?"
results = vector_store.similarity_search(query, k=3)  # Busca los 3 documentos más relevantes

# Muestra los resultados
for doc in results:
    print(doc.page_content)  # Muestra el contenido de los documentos
    print("---")

pip install -U langchain-openai

ls

ls

pip install pinecone-client langchain-pinecone

!pip install --upgrade langchain langchain-community

!pip install --upgrade langchain-openai

import os
print(os.getenv("OPENAI_API_KEY"))



import os
import getpass
import pinecone
import bs4
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import Pinecone
from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore


if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your API key: ")

if "PINECONE_API_KEY" not in os.environ:
    os.environ["PINECONE_API_KEY"] = getpass.getpass("Enter your Pinecone API key: ")

if "PINECONE_ENV" not in os.environ:
    os.environ["PINECONE_ENV"] = input("Enter your Pinecone environment: ")

index_name = "db"

pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"])


if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=1536,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region=os.environ["PINECONE_ENV"])
    )


loader = WebBaseLoader(
    web_paths=("https://es.wikipedia.org/wiki/BTS",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(["p", "h1", "h2"])
    ),
)
documents = loader.load()


text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
all_splits = text_splitter.split_documents(documents)


embedding_function = OpenAIEmbeddings()

vector_db = PineconeVectorStore.from_documents(all_splits, embedding_function, index_name=index_name)


retriever = vector_db.as_retriever()


llm = ChatOpenAI(model_name="gpt-4o-mini")


qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, return_source_documents=True)


query = "Explain me who is BTS"
result = qa_chain(query)


print("Respuesta:", result["result"])
print("Documentos fuente:", result["source_documents"])